{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d90c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, groupby\n",
    "from collections import Counter\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations, groupby\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "plt.style.use('bmh')\n",
    "\n",
    "path = '/Users/my_MAC/Coding/data/H_n_M_Kaggle/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv(path+'transactions_train.csv')\n",
    "articles = pd.read_csv(path+'articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b9df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bb67ae8",
   "metadata": {},
   "source": [
    "# Market Basket Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_article=pd.merge(transactions,articles[[\"article_id\",\"clean_prod_name\"\n",
    "                      ,\"product_type_name\",\"index_name\",\"product_group_name\"\n",
    "                     ]],how='inner',on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61dfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_article['Quantity'] = 1\n",
    "#tst1['type'] = pd.Series([\"omnichannel\" for x in range(len(tst1.index))])\n",
    "transaction_article.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_article1=transaction_article[[\"customer_id\",\"InvoiceDate\",\"clean_prod_name\",\"Quantity\",\"price\"]].drop_duplicates()\n",
    "transaction_article1.count()\n",
    "\n",
    "transaction_article1 = transaction_article.groupby([\"customer_id\",\"InvoiceDate\",\"clean_prod_name\"]).agg({\n",
    "    'Quantity': 'sum',\n",
    "    'price': 'sum'})\n",
    "\n",
    "transaction_article1=transaction_article1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87da2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_article1['_ID'] = transaction_article1['customer_id']  + transaction_article1['InvoiceDate'].astype(str) \n",
    "##+transactions['article_id'].astype(str) \n",
    "\n",
    "\n",
    "transaction_article1['Invoice_id'] = pd.factorize(transaction_article1['_ID'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef169e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers/transcations who buy same article in same time window, in different channels.\n",
    "agg_func_count = {'customer_id': [ 'nunique']}\n",
    "\n",
    "tst=transaction_article1.groupby(['Invoice_id']).agg(agg_func_count)\n",
    "tst.columns = tst.columns.to_flat_index().str.join('_')\n",
    "tst=tst.reset_index()\n",
    "tst1=tst.loc[tst['customer_id_nunique']!=1]\n",
    "tst1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = transaction_article1.groupby(['Invoice_id','clean_prod_name'])['Quantity'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket=basket.fillna(0)\n",
    "basket=basket.reset_index()\n",
    "basket.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b29aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders  = basket.set_index('Invoice_id')['clean_prod_name'].rename('order_id')\n",
    "display(orders.head(10))\n",
    "type(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd20002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns frequency counts for items and item pairs\n",
    "def freq(iterable):\n",
    "    if type(iterable) == pd.core.series.Series:\n",
    "        return iterable.value_counts().rename(\"freq\")\n",
    "    else: \n",
    "        return pd.Series(Counter(iterable)).rename(\"freq\")\n",
    "\n",
    "    \n",
    "# Returns number of unique orders\n",
    "def order_count(order_item):\n",
    "    return len(set(order_item.index))\n",
    "\n",
    "\n",
    "# Returns generator that yields item pairs, one at a time\n",
    "def get_item_pairs(order_item):\n",
    "    order_item = order_item.reset_index().to_numpy()\n",
    "    for order_id, order_object in groupby(order_item, lambda x: x[0]):\n",
    "        item_list = [item[1] for item in order_object]\n",
    "              \n",
    "        for item_pair in combinations(item_list, 2):\n",
    "            yield item_pair\n",
    "            \n",
    "\n",
    "# Returns frequency and support associated with item\n",
    "def merge_item_stats(item_pairs, item_stats):\n",
    "    return (item_pairs\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqA', 'support': 'supportA'}), left_on='item_A', right_index=True)\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqB', 'support': 'supportB'}), left_on='item_B', right_index=True))\n",
    "\n",
    "\n",
    "# Returns name associated with item\n",
    "def merge_item_name(rules, item_name):\n",
    "    columns = ['itemA','itemB','freqAB','supportAB','freqA','supportA','freqB','supportB', \n",
    "               'confidenceAtoB','confidenceBtoA','lift']\n",
    "    rules = (rules\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemA'}), left_on='item_A', right_on='item_id')\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemB'}), left_on='item_B', right_on='item_id'))\n",
    "    return rules[columns]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rules(order_item, min_support):\n",
    "\n",
    "    print(\"Starting order_item: {:22d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Calculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Filter from order_item items below min support \n",
    "    qualifying_items       = item_stats[item_stats['support'] >= min_support].index\n",
    "    order_item             = order_item[order_item.isin(qualifying_items)]\n",
    "\n",
    "    print(\"Items with support >= {}: {:15d}\".format(min_support, len(qualifying_items)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Filter from order_item orders with less than 2 items\n",
    "    order_size             = freq(order_item.index)\n",
    "    qualifying_orders      = order_size[order_size >= 2].index\n",
    "    order_item             = order_item[order_item.index.isin(qualifying_orders)]\n",
    "\n",
    "    print(\"Remaining orders with 2+ items: {:11d}\".format(len(qualifying_orders)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Recalculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Get item pairs generator\n",
    "    item_pair_gen          = get_item_pairs(order_item)\n",
    "\n",
    "\n",
    "    # Calculate item pair frequency and support\n",
    "    item_pairs              = freq(item_pair_gen).to_frame(\"freqAB\")\n",
    "    item_pairs['supportAB'] = item_pairs['freqAB'] / len(qualifying_orders) * 100\n",
    "\n",
    "    print(\"Item pairs: {:31d}\".format(len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Filter from item_pairs those below min support\n",
    "    item_pairs              = item_pairs[item_pairs['supportAB'] >= min_support]\n",
    "\n",
    "    print(\"Item pairs with support >= {}: {:10d}\\n\".format(min_support, len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Create table of association rules and compute relevant metrics\n",
    "    item_pairs = item_pairs.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n",
    "    item_pairs = merge_item_stats(item_pairs, item_stats)\n",
    "    \n",
    "    item_pairs['confidenceAtoB'] = item_pairs['supportAB'] / item_pairs['supportA']\n",
    "    item_pairs['confidenceBtoA'] = item_pairs['supportAB'] / item_pairs['supportB']\n",
    "    item_pairs['lift']           = item_pairs['supportAB'] / (item_pairs['supportA'] * item_pairs['supportB'])\n",
    "    \n",
    "    \n",
    "    # Return association rules sorted by lift in descending order\n",
    "    return item_pairs.sort_values('lift', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(orders, 0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    " for i,(k,v) in enumerate( pairs.items() ):\n",
    "        if i==50: break\n",
    " \n",
    "    \n",
    "        plt.figure(figsize=(20,5))\n",
    " \n",
    "        img1 = mpimg.imread(f'F:/images/0{str(v[\"article_id_A\"])[:2]}/0{int(v[\"article_id_A\"])}.jpg')\n",
    "        img2 = mpimg.imread(f'F:/images/0{str(v[\"article_id_B\"])[:2]}/0{int(v[\"article_id_B\"])}.jpg')\n",
    "        plt.subplot(1,4,1)\n",
    "        plt.title('When customers buy this',size=18)\n",
    "        plt.imshow(img1)\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.title('They buy this',size=18)\n",
    "        plt.imshow(img2)\n",
    "         \n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90681262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caf873df",
   "metadata": {},
   "source": [
    "# Item-Based Collaborative Filtering -using Probabilistic Matrix Factorization\n",
    "\n",
    "https://www.kaggle.com/code/luisrodri97/item-based-collaborative-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fcc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2020,9,1)\n",
    "# Filter transactions by date\n",
    "transactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\n",
    "transactions = transactions.loc[transactions[\"t_dat\"] >= start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e48aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter transactions by number of an article has been bought\n",
    "article_bought_count = transactions[['article_id', 't_dat']].groupby('article_id').count().reset_index().rename(columns={'t_dat': 'count'})\n",
    "most_bought_articles = article_bought_count[article_bought_count['count']>10]['article_id'].values\n",
    "transactions = transactions[transactions['article_id'].isin(most_bought_articles)]\n",
    "transactions[\"bought\"]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933bdf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative samples\n",
    "np.random.seed(0)\n",
    "\n",
    "negative_samples = pd.DataFrame({\n",
    "    'article_id': np.random.choice(transactions.article_id.unique(), transactions.shape[0]),\n",
    "    'customer_id': np.random.choice(transactions.customer_id.unique(), transactions.shape[0]),\n",
    "    'bought': np.zeros(transactions.shape[0])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a540ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class ItemBased_RecSys:\n",
    "    ''' Collaborative filtering using a custom sim(u,u'). '''\n",
    "\n",
    "    def __init__(self, positive_transactions, negative_transactions, num_components=10):\n",
    "        ''' Constructor '''\n",
    "        self.positive_transactions = positive_transactions\n",
    "        self.transactions = pd.concat([positive_transactions, negative_transactions])\n",
    "        self.customers = self.transactions.customer_id.values\n",
    "        self.articles = self.transactions.article_id.values\n",
    "        self.bought = self.transactions.bought.values\n",
    "        self.num_components = num_components\n",
    "\n",
    "        self.customer_id2index = {c: i for i, c in enumerate(np.unique(self.customers))}\n",
    "        self.article_id2index = {a: i for i, a in enumerate(np.unique(self.articles))}\n",
    "        \n",
    "    def __sdg__(self):\n",
    "        for idx in tqdm(self.training_indices):\n",
    "            # Get the current sample\n",
    "            customer_id = self.customers[idx]\n",
    "            article_id = self.articles[idx]\n",
    "            bought = self.bought[idx]\n",
    "\n",
    "            # Get the index of the user and the article\n",
    "            customer_index = self.customer_id2index[customer_id]\n",
    "            article_index = self.article_id2index[article_id]\n",
    "\n",
    "            # Compute the prediction and the error\n",
    "            prediction = self.predict_single(customer_index, article_index)\n",
    "            error = (bought - prediction) # error\n",
    "            \n",
    "            # Update latent factors in terms of the learning rate and the observed error\n",
    "            self.customers_latent_matrix[customer_index] += self.learning_rate * \\\n",
    "                                    (error * self.articles_latent_matrix[article_index] - \\\n",
    "                                     self.lmbda * self.customers_latent_matrix[customer_index])\n",
    "            self.articles_latent_matrix[article_index] += self.learning_rate * \\\n",
    "                                    (error * self.customers_latent_matrix[customer_index] - \\\n",
    "                                     self.lmbda * self.articles_latent_matrix[article_index])\n",
    "                \n",
    "                \n",
    "    def fit(self, n_epochs=10, learning_rate=0.001, lmbda=0.1):\n",
    "        ''' Compute the matrix factorization R = P x Q '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lmbda = lmbda\n",
    "        n_samples = self.transactions.shape[0]\n",
    "        \n",
    "        # Initialize latent matrices\n",
    "        self.customers_latent_matrix = np.random.normal(scale=1., size=(len(np.unique(self.customers)), self.num_components))\n",
    "        self.articles_latent_matrix = np.random.normal(scale=1., size=(len(np.unique(self.articles)), self.num_components))\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            print('Epoch: {}'.format(epoch))\n",
    "            self.training_indices = np.arange(n_samples)\n",
    "            \n",
    "            # Shuffle training samples and follow stochastic gradient descent\n",
    "            np.random.shuffle(self.training_indices)\n",
    "            self.__sdg__()\n",
    "\n",
    "    def predict_single(self, customer_index, article_index):\n",
    "        ''' Make a prediction for an specific user and article '''\n",
    "        prediction = np.dot(self.customers_latent_matrix[customer_index], self.articles_latent_matrix[article_index])\n",
    "        prediction = np.clip(prediction, 0, 1)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    def default_recommendation(self):\n",
    "        ''' Calculate time decaying popularity '''\n",
    "        # Calculate time decaying popularity. This leads to items bought more recently having more weight in the popularity list.\n",
    "        # In simple words, item A bought 5 times on the first day of the train period is inferior than item B bought 4 times on the last day of the train period.\n",
    "        self.positive_transactions['pop_factor'] = self.positive_transactions['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,23) - x).days)\n",
    "        transactions_by_article = self.positive_transactions[['article_id', 'pop_factor']].groupby('article_id').sum().reset_index()\n",
    "        return transactions_by_article.sort_values(by='pop_factor', ascending=False)['article_id'].values[:12]\n",
    "\n",
    "\n",
    "    def predict(self, customers):\n",
    "        ''' Make recommendations '''\n",
    "        recommendations = []\n",
    "        self.articles_latent_matrix[np.isnan(self.articles_latent_matrix)] = 0\n",
    "        # Compute similarity matrix (cosine)\n",
    "        similarity_matrix = cosine_similarity(self.articles_latent_matrix, self.articles_latent_matrix, dense_output=False)\n",
    "\n",
    "        # Convert similarity matrix into a matrix containing the 12 most similar items' index for each item\n",
    "        similarity_matrix = np.argsort(similarity_matrix, axis=1)\n",
    "        similarity_matrix = similarity_matrix[:, -12:]\n",
    "\n",
    "        # Get default recommendation (time decay popularity)\n",
    "        default_recommendation = self.default_recommendation()\n",
    "\n",
    "        # Group articles by user and articles to compute the number of times each article has been bought by each user\n",
    "        transactions_by_customer = self.positive_transactions[['customer_id', 'article_id', 'bought']].groupby(['customer_id', 'article_id']).count().reset_index()\n",
    "        most_bought_article = transactions_by_customer.loc[transactions_by_customer.groupby('customer_id').bought.idxmax()]['article_id'].values\n",
    "\n",
    "        # Make predictions\n",
    "        for customer in tqdm(customers):\n",
    "            try:\n",
    "                rec_aux1 = []\n",
    "                rec_aux2 = []\n",
    "                aux = []\n",
    "\n",
    "                # Retrieve the most bought article by customer\n",
    "                user_most_bought_article_id = most_bought_article[self.customer_id2index[customer]]\n",
    "\n",
    "                # Using the similarity matrix, get the 6 most similar articles\n",
    "                rec_aux1 = self.articles[similarity_matrix[self.article_id2index[user_most_bought_article_id]]]\n",
    "                # Return the half of the default recommendation\n",
    "                rec_aux2 = default_recommendation\n",
    "\n",
    "                # Merge half of both recommendation lists\n",
    "                for rec_idx in range(6):\n",
    "                    aux.append(rec_aux2[rec_idx])\n",
    "                    aux.append(rec_aux1[rec_idx])\n",
    "\n",
    "                recommendations.append(' '.join(aux))\n",
    "            except:\n",
    "                # Return the default recommendation\n",
    "                recommendations.append(' '.join(default_recommendation))\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'customer_id': customers,\n",
    "            'prediction': recommendations,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = ItemBased_RecSys(transactions, negative_samples, num_components=1000)\n",
    "rec.fit(n_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = rec.predict(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4adc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2aba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45b510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd42e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
